#!/usr/bin/env python3
"""
End-to-End Load Test Harness for Analyst Augmentation Agent

This comprehensive load testing suite validates the entire blockchain analysis stack
under realistic production workloads. It tests all monitoring, tracing, and protection
systems while generating actionable performance insights.

Features:
- Multi-scenario load testing (fraud detection, portfolio analysis, whale tracking)
- Provider integration testing (SIM, Covalent, Moralis)
- Back-pressure middleware validation
- Circuit breaker and rate limiting testing
- OpenTelemetry tracing validation
- Grafana metrics verification
- Performance benchmarking with realistic crypto workloads
- Automated reporting and recommendations

Usage:
    python scripts/end_to_end_load_test.py
    python scripts/end_to_end_load_test.py --scenario fraud_detection --users 50
    python scripts/end_to_end_load_test.py --duration 300 --report-format html
    python scripts/end_to_end_load_test.py --stress-test --max-users 200

Generated by Factory Droid on 2025-06-28 following "cook & push to GitHub" motto
"""

import argparse
import asyncio
import json
import logging
import os
import random
import statistics
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urljoin

import aiohttp
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class LoadTestConfig:
    """Configuration for load testing scenarios."""
    base_url: str = "http://localhost:8000"
    concurrent_users: int = 10
    test_duration: int = 60  # seconds
    ramp_up_time: int = 10   # seconds
    scenario: str = "comprehensive"
    api_key: Optional[str] = None
    stress_test: bool = False
    max_users: int = 100
    report_format: str = "json"  # json, html, console
    output_dir: str = "load_test_results"

@dataclass
class TestResult:
    """Individual test result."""
    scenario: str
    endpoint: str
    method: str
    status_code: int
    response_time: float
    success: bool
    error: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)
    provider: Optional[str] = None

@dataclass
class LoadTestResults:
    """Aggregated load test results."""
    config: LoadTestConfig
    start_time: datetime
    end_time: datetime
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    response_times: List[float] = field(default_factory=list)
    error_counts: Dict[str, int] = field(default_factory=dict)
    provider_stats: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    scenario_stats: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    results: List[TestResult] = field(default_factory=list)

# Test scenarios with realistic blockchain analysis workloads
TEST_SCENARIOS = {
    "fraud_detection": {
        "description": "Fraud detection workflow with wallet analysis",
        "endpoints": [
            {"path": "/api/v1/analysis/fraud-score", "method": "POST", "weight": 0.4},
            {"path": "/api/v1/graph/wallet-risk", "method": "GET", "weight": 0.3},
            {"path": "/api/v1/crew/fraud-analysis", "method": "POST", "weight": 0.3},
        ],
        "wallet_addresses": [
            "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",  # Vitalik
            "0x742db678967cd83c2fb53ea1ac84b29b3ce1fcdf",   # Random
            "0x1f9840a85d5af5bf1d1762f925bdaddc4201f984",   # Uniswap
            "0xa0b86a33e6776bb0a6f4f8a5bbce0e0d0e5e8c5f",   # Random
        ]
    },
    "portfolio_analysis": {
        "description": "Portfolio tracking and balance analysis",
        "endpoints": [
            {"path": "/api/v1/analysis/portfolio", "method": "GET", "weight": 0.5},
            {"path": "/api/v1/tools/sim_balances", "method": "POST", "weight": 0.3},
            {"path": "/api/v1/tools/covalent_balances", "method": "POST", "weight": 0.2},
        ],
        "wallet_addresses": [
            "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "0x742db678967cd83c2fb53ea1ac84b29b3ce1fcdf",
            "0x1f9840a85d5af5bf1d1762f925bdaddc4201f984",
        ]
    },
    "whale_tracking": {
        "description": "Large transaction and whale detection",
        "endpoints": [
            {"path": "/api/v1/whales/detect", "method": "POST", "weight": 0.4},
            {"path": "/api/v1/analysis/large-transfers", "method": "GET", "weight": 0.3},
            {"path": "/api/v1/tools/sim_activity", "method": "POST", "weight": 0.3},
        ],
        "wallet_addresses": [
            "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "0x742db678967cd83c2fb53ea1ac84b29b3ce1fcdf",
        ]
    },
    "comprehensive": {
        "description": "Mixed workload across all features",
        "endpoints": [
            {"path": "/api/v1/analysis/fraud-score", "method": "POST", "weight": 0.2},
            {"path": "/api/v1/analysis/portfolio", "method": "GET", "weight": 0.2},
            {"path": "/api/v1/tools/sim_balances", "method": "POST", "weight": 0.15},
            {"path": "/api/v1/tools/covalent_balances", "method": "POST", "weight": 0.15},
            {"path": "/api/v1/tools/moralis_nft", "method": "POST", "weight": 0.1},
            {"path": "/api/v1/graph/wallet-risk", "method": "GET", "weight": 0.1},
            {"path": "/api/v1/whales/detect", "method": "POST", "weight": 0.08},
        ],
        "wallet_addresses": [
            "0xd8da6bf26964af9d7eed9e03e53415d37aa96045",
            "0x742db678967cd83c2fb53ea1ac84b29b3ce1fcdf",
            "0x1f9840a85d5af5bf1d1762f925bdaddc4201f984",
            "0xa0b86a33e6776bb0a6f4f8a5bbce0e0d0e5e8c5f",
        ]
    }
}

class LoadTestClient:
    """HTTP client for load testing with retry logic."""
    
    def __init__(self, base_url: str, api_key: Optional[str] = None):
        self.base_url = base_url
        self.session = requests.Session()
        
        # Configure retry strategy
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS", "POST"],
            backoff_factor=1
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Set headers
        self.session.headers.update({
            "Content-Type": "application/json",
            "User-Agent": "LoadTest/1.0"
        })
        
        if api_key:
            self.session.headers["Authorization"] = f"Bearer {api_key}"
    
    def make_request(self, endpoint: Dict[str, Any], wallet_address: str) -> TestResult:
        """Make a single request and return the result."""
        start_time = time.time()
        path = endpoint["path"]
        method = endpoint["method"]
        
        # Build request payload based on endpoint
        payload = self._build_payload(path, wallet_address)
        
        try:
            url = urljoin(self.base_url, path)
            
            if method.upper() == "GET":
                response = self.session.get(url, params=payload, timeout=30)
            else:
                response = self.session.post(url, json=payload, timeout=30)
            
            response_time = time.time() - start_time
            
            # Detect provider from response or path
            provider = self._detect_provider(path, response)
            
            return TestResult(
                scenario="unknown",
                endpoint=path,
                method=method,
                status_code=response.status_code,
                response_time=response_time,
                success=200 <= response.status_code < 400,
                provider=provider,
                timestamp=datetime.now()
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return TestResult(
                scenario="unknown",
                endpoint=path,
                method=method,
                status_code=0,
                response_time=response_time,
                success=False,
                error=str(e),
                timestamp=datetime.now()
            )
    
    def _build_payload(self, path: str, wallet_address: str) -> Dict[str, Any]:
        """Build request payload based on endpoint."""
        if "fraud-score" in path:
            return {"wallet_address": wallet_address, "analysis_depth": "standard"}
        elif "portfolio" in path:
            return {"address": wallet_address}
        elif "balances" in path:
            return {"address": wallet_address, "metadata": "url,logo"}
        elif "activity" in path:
            return {"address": wallet_address, "limit": 25}
        elif "nft" in path:
            return {"address": wallet_address, "limit": 50}
        elif "whale" in path:
            return {"addresses": [wallet_address], "threshold_usd": 100000}
        elif "wallet-risk" in path:
            return {"address": wallet_address}
        else:
            return {"address": wallet_address}
    
    def _detect_provider(self, path: str, response: requests.Response) -> Optional[str]:
        """Detect which provider was used for this request."""
        if "sim" in path:
            return "sim"
        elif "covalent" in path:
            return "covalent"
        elif "moralis" in path:
            return "moralis"
        
        # Try to detect from response headers or content
        try:
            if hasattr(response, 'json'):
                data = response.json()
                if isinstance(data, dict) and "provider" in data:
                    return data["provider"]
        except:
            pass
        
        return None

class LoadTestRunner:
    """Main load test runner."""
    
    def __init__(self, config: LoadTestConfig):
        self.config = config
        self.results = LoadTestResults(
            config=config,
            start_time=datetime.now(),
            end_time=datetime.now()
        )
        
        # Ensure output directory exists
        os.makedirs(config.output_dir, exist_ok=True)
    
    def run_load_test(self) -> LoadTestResults:
        """Run the complete load test."""
        logger.info(f"Starting load test: {self.config.scenario}")
        logger.info(f"Users: {self.config.concurrent_users}, Duration: {self.config.test_duration}s")
        
        self.results.start_time = datetime.now()
        
        # Get scenario configuration
        scenario_config = TEST_SCENARIOS.get(self.config.scenario)
        if not scenario_config:
            raise ValueError(f"Unknown scenario: {self.config.scenario}")
        
        try:
            if self.config.stress_test:
                self._run_stress_test(scenario_config)
            else:
                self._run_standard_test(scenario_config)
            
            self.results.end_time = datetime.now()
            self._calculate_statistics()
            
            logger.info("Load test completed successfully")
            return self.results
            
        except Exception as e:
            logger.error(f"Load test failed: {e}")
            self.results.end_time = datetime.now()
            raise
    
    def _run_standard_test(self, scenario_config: Dict[str, Any]) -> None:
        """Run standard load test with fixed concurrency."""
        with ThreadPoolExecutor(max_workers=self.config.concurrent_users) as executor:
            # Submit user simulation tasks
            futures = []
            for user_id in range(self.config.concurrent_users):
                # Stagger user start times (ramp-up)
                delay = (user_id / self.config.concurrent_users) * self.config.ramp_up_time
                future = executor.submit(self._simulate_user, user_id, scenario_config, delay)
                futures.append(future)
            
            # Collect results
            for future in as_completed(futures):
                try:
                    user_results = future.result()
                    self.results.results.extend(user_results)
                except Exception as e:
                    logger.error(f"User simulation failed: {e}")
    
    def _run_stress_test(self, scenario_config: Dict[str, Any]) -> None:
        """Run stress test with gradually increasing load."""
        logger.info("Running stress test with increasing load")
        
        step_duration = self.config.test_duration // 5  # 5 load steps
        current_users = self.config.concurrent_users
        
        for step in range(5):
            step_users = min(current_users, self.config.max_users)
            logger.info(f"Stress test step {step + 1}/5: {step_users} users")
            
            with ThreadPoolExecutor(max_workers=step_users) as executor:
                futures = []
                for user_id in range(step_users):
                    future = executor.submit(
                        self._simulate_user, 
                        user_id, 
                        scenario_config, 
                        0,  # No ramp-up in stress test
                        step_duration
                    )
                    futures.append(future)
                
                for future in as_completed(futures):
                    try:
                        user_results = future.result()
                        self.results.results.extend(user_results)
                    except Exception as e:
                        logger.error(f"Stress test user failed: {e}")
            
            current_users = int(current_users * 1.5)  # Increase load by 50% each step
    
    def _simulate_user(
        self, 
        user_id: int, 
        scenario_config: Dict[str, Any], 
        start_delay: float = 0,
        duration_override: Optional[int] = None
    ) -> List[TestResult]:
        """Simulate a single user's behavior."""
        time.sleep(start_delay)
        
        client = LoadTestClient(self.config.base_url, self.config.api_key)
        user_results = []
        
        test_duration = duration_override or self.config.test_duration
        end_time = time.time() + test_duration
        
        while time.time() < end_time:
            # Select random endpoint based on weights
            endpoint = self._select_weighted_endpoint(scenario_config["endpoints"])
            wallet_address = random.choice(scenario_config["wallet_addresses"])
            
            # Make request
            result = client.make_request(endpoint, wallet_address)
            result.scenario = self.config.scenario
            user_results.append(result)
            
            # Add some think time between requests (0.5-2 seconds)
            think_time = random.uniform(0.5, 2.0)
            time.sleep(think_time)
        
        logger.debug(f"User {user_id} completed with {len(user_results)} requests")
        return user_results
    
    def _select_weighted_endpoint(self, endpoints: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Select an endpoint based on weights."""
        weights = [ep.get("weight", 1.0) for ep in endpoints]
        return random.choices(endpoints, weights=weights, k=1)[0]
    
    def _calculate_statistics(self) -> None:
        """Calculate test statistics."""
        self.results.total_requests = len(self.results.results)
        self.results.successful_requests = sum(1 for r in self.results.results if r.success)
        self.results.failed_requests = self.results.total_requests - self.results.successful_requests
        
        # Response times
        self.results.response_times = [r.response_time for r in self.results.results]
        
        # Error counts
        error_counts = {}
        for result in self.results.results:
            if not result.success:
                error_key = f"{result.status_code}_{result.error or 'unknown'}"
                error_counts[error_key] = error_counts.get(error_key, 0) + 1
        self.results.error_counts = error_counts
        
        # Provider statistics
        provider_stats = {}
        for result in self.results.results:
            provider = result.provider or "unknown"
            if provider not in provider_stats:
                provider_stats[provider] = {
                    "total_requests": 0,
                    "successful_requests": 0,
                    "response_times": []
                }
            
            provider_stats[provider]["total_requests"] += 1
            if result.success:
                provider_stats[provider]["successful_requests"] += 1
            provider_stats[provider]["response_times"].append(result.response_time)
        
        self.results.provider_stats = provider_stats
        
        # Scenario statistics
        scenario_stats = {}
        for result in self.results.results:
            scenario = result.scenario
            if scenario not in scenario_stats:
                scenario_stats[scenario] = {
                    "total_requests": 0,
                    "successful_requests": 0,
                    "response_times": []
                }
            
            scenario_stats[scenario]["total_requests"] += 1
            if result.success:
                scenario_stats[scenario]["successful_requests"] += 1
            scenario_stats[scenario]["response_times"].append(result.response_time)
        
        self.results.scenario_stats = scenario_stats

class LoadTestReporter:
    """Generate load test reports."""
    
    def __init__(self, results: LoadTestResults):
        self.results = results
    
    def generate_report(self, format_type: str = "json") -> str:
        """Generate test report in specified format."""
        if format_type == "json":
            return self._generate_json_report()
        elif format_type == "html":
            return self._generate_html_report()
        elif format_type == "console":
            return self._generate_console_report()
        else:
            raise ValueError(f"Unsupported report format: {format_type}")
    
    def _generate_json_report(self) -> str:
        """Generate JSON report."""
        report_data = {
            "test_config": {
                "scenario": self.results.config.scenario,
                "concurrent_users": self.results.config.concurrent_users,
                "test_duration": self.results.config.test_duration,
                "stress_test": self.results.config.stress_test
            },
            "test_summary": {
                "start_time": self.results.start_time.isoformat(),
                "end_time": self.results.end_time.isoformat(),
                "total_duration": (self.results.end_time - self.results.start_time).total_seconds(),
                "total_requests": self.results.total_requests,
                "successful_requests": self.results.successful_requests,
                "failed_requests": self.results.failed_requests,
                "success_rate": (self.results.successful_requests / max(self.results.total_requests, 1)) * 100,
                "requests_per_second": self.results.total_requests / max((self.results.end_time - self.results.start_time).total_seconds(), 1)
            },
            "performance_metrics": self._calculate_performance_metrics(),
            "provider_stats": self._calculate_provider_metrics(),
            "error_analysis": self.results.error_counts,
            "recommendations": self._generate_recommendations()
        }
        
        return json.dumps(report_data, indent=2, default=str)
    
    def _generate_html_report(self) -> str:
        """Generate HTML report."""
        performance_metrics = self._calculate_performance_metrics()
        provider_metrics = self._calculate_provider_metrics()
        
        html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Load Test Report - {self.results.config.scenario}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f5f5f5; padding: 20px; border-radius: 5px; }}
        .metric {{ display: inline-block; margin: 10px; padding: 15px; background: #e9ecef; border-radius: 5px; }}
        .success {{ color: #28a745; }}
        .warning {{ color: #ffc107; }}
        .error {{ color: #dc3545; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Load Test Report</h1>
        <p><strong>Scenario:</strong> {self.results.config.scenario}</p>
        <p><strong>Duration:</strong> {(self.results.end_time - self.results.start_time).total_seconds():.1f} seconds</p>
        <p><strong>Users:</strong> {self.results.config.concurrent_users}</p>
    </div>
    
    <h2>Performance Summary</h2>
    <div class="metric">
        <strong>Total Requests:</strong> {self.results.total_requests}
    </div>
    <div class="metric success">
        <strong>Success Rate:</strong> {(self.results.successful_requests / max(self.results.total_requests, 1)) * 100:.1f}%
    </div>
    <div class="metric">
        <strong>Avg Response Time:</strong> {performance_metrics.get('avg_response_time', 0):.3f}s
    </div>
    <div class="metric">
        <strong>95th Percentile:</strong> {performance_metrics.get('p95_response_time', 0):.3f}s
    </div>
    
    <h2>Provider Performance</h2>
    <table>
        <tr>
            <th>Provider</th>
            <th>Requests</th>
            <th>Success Rate</th>
            <th>Avg Response Time</th>
            <th>95th Percentile</th>
        </tr>
        {self._format_provider_table_rows(provider_metrics)}
    </table>
    
    <h2>Error Analysis</h2>
    <table>
        <tr><th>Error Type</th><th>Count</th></tr>
        {self._format_error_table_rows()}
    </table>
    
    <h2>Recommendations</h2>
    <ul>
        {self._format_recommendations_list()}
    </ul>
</body>
</html>
        """
        
        return html_template
    
    def _generate_console_report(self) -> str:
        """Generate console-friendly report."""
        performance_metrics = self._calculate_performance_metrics()
        provider_metrics = self._calculate_provider_metrics()
        
        report = f"""
===============================================================================
                            LOAD TEST REPORT
===============================================================================

Test Configuration:
  Scenario: {self.results.config.scenario}
  Users: {self.results.config.concurrent_users}
  Duration: {(self.results.end_time - self.results.start_time).total_seconds():.1f}s
  Stress Test: {'Yes' if self.results.config.stress_test else 'No'}

Performance Summary:
  Total Requests: {self.results.total_requests}
  Successful: {self.results.successful_requests} ({(self.results.successful_requests / max(self.results.total_requests, 1)) * 100:.1f}%)
  Failed: {self.results.failed_requests} ({(self.results.failed_requests / max(self.results.total_requests, 1)) * 100:.1f}%)
  Requests/Second: {self.results.total_requests / max((self.results.end_time - self.results.start_time).total_seconds(), 1):.2f}

Response Time Statistics:
  Average: {performance_metrics.get('avg_response_time', 0):.3f}s
  Median: {performance_metrics.get('median_response_time', 0):.3f}s
  95th Percentile: {performance_metrics.get('p95_response_time', 0):.3f}s
  99th Percentile: {performance_metrics.get('p99_response_time', 0):.3f}s
  Max: {performance_metrics.get('max_response_time', 0):.3f}s

Provider Performance:
{self._format_provider_console_stats(provider_metrics)}

Error Analysis:
{self._format_error_console_stats()}

Recommendations:
{self._format_recommendations_console()}

===============================================================================
        """
        
        return report.strip()
    
    def _calculate_performance_metrics(self) -> Dict[str, float]:
        """Calculate performance metrics."""
        if not self.results.response_times:
            return {}
        
        response_times = sorted(self.results.response_times)
        
        return {
            "avg_response_time": statistics.mean(response_times),
            "median_response_time": statistics.median(response_times),
            "p95_response_time": response_times[int(len(response_times) * 0.95)] if response_times else 0,
            "p99_response_time": response_times[int(len(response_times) * 0.99)] if response_times else 0,
            "max_response_time": max(response_times),
            "min_response_time": min(response_times)
        }
    
    def _calculate_provider_metrics(self) -> Dict[str, Dict[str, Any]]:
        """Calculate per-provider metrics."""
        provider_metrics = {}
        
        for provider, stats in self.results.provider_stats.items():
            if stats["response_times"]:
                response_times = sorted(stats["response_times"])
                provider_metrics[provider] = {
                    "total_requests": stats["total_requests"],
                    "successful_requests": stats["successful_requests"],
                    "success_rate": (stats["successful_requests"] / max(stats["total_requests"], 1)) * 100,
                    "avg_response_time": statistics.mean(response_times),
                    "p95_response_time": response_times[int(len(response_times) * 0.95)] if response_times else 0,
                    "p99_response_time": response_times[int(len(response_times) * 0.99)] if response_times else 0,
                }
        
        return provider_metrics
    
    def _generate_recommendations(self) -> List[str]:
        """Generate performance recommendations."""
        recommendations = []
        performance_metrics = self._calculate_performance_metrics()
        
        # Response time recommendations
        avg_response_time = performance_metrics.get('avg_response_time', 0)
        if avg_response_time > 2.0:
            recommendations.append("Average response time is high (>2s). Consider optimizing database queries and external API calls.")
        
        p95_response_time = performance_metrics.get('p95_response_time', 0)
        if p95_response_time > 5.0:
            recommendations.append("95th percentile response time is concerning (>5s). Investigate slow endpoints and consider caching.")
        
        # Success rate recommendations
        success_rate = (self.results.successful_requests / max(self.results.total_requests, 1)) * 100
        if success_rate < 95:
            recommendations.append(f"Success rate is below 95% ({success_rate:.1f}%). Review error logs and improve error handling.")
        
        # Provider-specific recommendations
        for provider, metrics in self._calculate_provider_metrics().items():
            if metrics["success_rate"] < 90:
                recommendations.append(f"Provider '{provider}' has low success rate ({metrics['success_rate']:.1f}%). Check API limits and error handling.")
        
        # Error-based recommendations
        if "429" in str(self.results.error_counts):
            recommendations.append("Rate limiting detected (HTTP 429). Consider implementing back-pressure or reducing request rates.")
        
        if not recommendations:
            recommendations.append("Performance looks good! Consider running longer tests or higher concurrency to find limits.")
        
        return recommendations
    
    def _format_provider_table_rows(self, provider_metrics: Dict[str, Dict[str, Any]]) -> str:
        """Format provider metrics as HTML table rows."""
        rows = []
        for provider, metrics in provider_metrics.items():
            rows.append(f"""
                <tr>
                    <td>{provider}</td>
                    <td>{metrics['total_requests']}</td>
                    <td>{metrics['success_rate']:.1f}%</td>
                    <td>{metrics['avg_response_time']:.3f}s</td>
                    <td>{metrics['p95_response_time']:.3f}s</td>
                </tr>
            """)
        return "".join(rows)
    
    def _format_error_table_rows(self) -> str:
        """Format error counts as HTML table rows."""
        rows = []
        for error, count in self.results.error_counts.items():
            rows.append(f"<tr><td>{error}</td><td>{count}</td></tr>")
        return "".join(rows)
    
    def _format_recommendations_list(self) -> str:
        """Format recommendations as HTML list items."""
        recommendations = self._generate_recommendations()
        return "".join(f"<li>{rec}</li>" for rec in recommendations)
    
    def _format_provider_console_stats(self, provider_metrics: Dict[str, Dict[str, Any]]) -> str:
        """Format provider stats for console output."""
        if not provider_metrics:
            return "  No provider data available"
        
        lines = []
        for provider, metrics in provider_metrics.items():
            lines.append(f"  {provider}:")
            lines.append(f"    Requests: {metrics['total_requests']}")
            lines.append(f"    Success Rate: {metrics['success_rate']:.1f}%")
            lines.append(f"    Avg Response Time: {metrics['avg_response_time']:.3f}s")
            lines.append("")
        
        return "\n".join(lines)
    
    def _format_error_console_stats(self) -> str:
        """Format error stats for console output."""
        if not self.results.error_counts:
            return "  No errors detected"
        
        lines = []
        for error, count in self.results.error_counts.items():
            lines.append(f"  {error}: {count}")
        
        return "\n".join(lines)
    
    def _format_recommendations_console(self) -> str:
        """Format recommendations for console output."""
        recommendations = self._generate_recommendations()
        return "\n".join(f"  - {rec}" for rec in recommendations)
    
    def save_report(self, output_path: str, format_type: str) -> None:
        """Save report to file."""
        report_content = self.generate_report(format_type)
        
        with open(output_path, 'w') as f:
            f.write(report_content)
        
        logger.info(f"Report saved to: {output_path}")

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="End-to-end load test for blockchain analysis platform")
    parser.add_argument("--base-url", default="http://localhost:8000",
                       help="Base URL of the API")
    parser.add_argument("--users", type=int, default=10,
                       help="Number of concurrent users")
    parser.add_argument("--duration", type=int, default=60,
                       help="Test duration in seconds")
    parser.add_argument("--ramp-up", type=int, default=10,
                       help="Ramp-up time in seconds")
    parser.add_argument("--scenario", choices=list(TEST_SCENARIOS.keys()), default="comprehensive",
                       help="Load test scenario")
    parser.add_argument("--stress-test", action="store_true",
                       help="Run stress test with increasing load")
    parser.add_argument("--max-users", type=int, default=100,
                       help="Maximum users for stress test")
    parser.add_argument("--report-format", choices=["json", "html", "console"], default="console",
                       help="Report output format")
    parser.add_argument("--output-dir", default="load_test_results",
                       help="Output directory for reports")
    parser.add_argument("--api-key", help="API key for authentication")
    
    args = parser.parse_args()
    
    # Create configuration
    config = LoadTestConfig(
        base_url=args.base_url,
        concurrent_users=args.users,
        test_duration=args.duration,
        ramp_up_time=args.ramp_up,
        scenario=args.scenario,
        api_key=args.api_key,
        stress_test=args.stress_test,
        max_users=args.max_users,
        report_format=args.report_format,
        output_dir=args.output_dir
    )
    
    print(f"ðŸš€ Starting load test: {config.scenario}")
    print(f"ðŸ“Š Configuration: {config.concurrent_users} users, {config.test_duration}s duration")
    
    try:
        # Run load test
        runner = LoadTestRunner(config)
        results = runner.run_load_test()
        
        # Generate and display report
        reporter = LoadTestReporter(results)
        
        if config.report_format == "console":
            print(reporter.generate_report("console"))
        
        # Save detailed reports
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save JSON report
        json_path = os.path.join(config.output_dir, f"load_test_{timestamp}.json")
        reporter.save_report(json_path, "json")
        
        # Save HTML report
        html_path = os.path.join(config.output_dir, f"load_test_{timestamp}.html")
        reporter.save_report(html_path, "html")
        
        print(f"\nðŸ“‹ Detailed reports saved:")
        print(f"   JSON: {json_path}")
        print(f"   HTML: {html_path}")
        
        # Exit with error code if test had issues
        success_rate = (results.successful_requests / max(results.total_requests, 1)) * 100
        if success_rate < 95:
            print(f"\nâš ï¸  Warning: Success rate below 95% ({success_rate:.1f}%)")
            return 1
        
        print(f"\nâœ… Load test completed successfully! Success rate: {success_rate:.1f}%")
        return 0
        
    except Exception as e:
        logger.error(f"Load test failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
